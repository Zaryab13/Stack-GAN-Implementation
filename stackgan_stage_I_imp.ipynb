{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrrajatgarg/StackGAN/blob/master/stackgan_stage_I_imp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fBRj1Lxsle4H"
      },
      "source": [
        "# Stage I GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "psC9UHjzliA1"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "ml58N7F-jnLh",
        "outputId": "e2dd3ad8-ccd0-4c1d-bee9-bf5ab682881a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import Input, Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
        "    concatenate, Flatten, Lambda, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XZybHodapH3H"
      },
      "source": [
        "# Loading of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eMvLOjXalnKd"
      },
      "outputs": [],
      "source": [
        "def load_class_ids(class_info_file_path):\n",
        "    \"\"\"\n",
        "    Load class ids from class_info.pickle file\n",
        "    \"\"\"\n",
        "    with open(class_info_file_path, 'rb') as f:\n",
        "        class_ids = pickle.load(f, encoding='latin1')\n",
        "        return class_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ifdCVSGco5pD"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(embeddings_file_path):\n",
        "    \"\"\"\n",
        "    Load embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f, encoding='latin1')\n",
        "        embeddings = np.array(embeddings)\n",
        "        print('embeddings: ', embeddings.shape)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fkwKbQgvo7pG"
      },
      "outputs": [],
      "source": [
        "def load_filenames(filenames_file_path):\n",
        "    \"\"\"\n",
        "    Load filenames.pickle file and return a list of all file names\n",
        "    \"\"\"\n",
        "    with open(filenames_file_path, 'rb') as f:\n",
        "        filenames = pickle.load(f, encoding='latin1')\n",
        "    return filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YxlUAsjuo-i5"
      },
      "outputs": [],
      "source": [
        "def load_bounding_boxes(dataset_dir):\n",
        "    \"\"\"\n",
        "    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n",
        "    \"\"\"\n",
        "    # Paths\n",
        "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
        "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
        "\n",
        "    # Read bounding_boxes.txt and images.txt file\n",
        "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
        "                                    delim_whitespace=True, header=None).astype(int)\n",
        "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
        "\n",
        "    # Create a list of file names\n",
        "    file_names = df_file_names[1].tolist()\n",
        "\n",
        "    # Create a dictionary of file_names and bounding boxes\n",
        "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
        "\n",
        "    # Assign a bounding box to the corresponding image\n",
        "    for i in range(0, len(file_names)):\n",
        "        # Get the bounding box\n",
        "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "        key = file_names[i][:-4]\n",
        "        filename_boundingbox_dict[key] = bounding_box\n",
        "\n",
        "    return filename_boundingbox_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NQf-MsWtpAyk"
      },
      "outputs": [],
      "source": [
        "def get_img(img_path, bbox, image_size):\n",
        "    \"\"\"\n",
        "    Load and resize image\n",
        "    \"\"\"\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "    if bbox is not None:\n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.minimum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.minimum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2])\n",
        "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk-4oF83Su2v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BEtbSBB8pCfo"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    filenames = load_filenames(filenames_file_path)\n",
        "    class_ids = load_class_ids(class_info_file_path)\n",
        "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
        "    all_embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "    X, y, embeddings = [], [], []\n",
        "\n",
        "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
        "\n",
        "    for index, filename in enumerate(filenames):\n",
        "        bounding_box = bounding_boxes[filename]\n",
        "\n",
        "        try:\n",
        "            # Load images\n",
        "            img_name = '{}/images/{}.jpg'.format(cub_dataset_dir, filename)\n",
        "            img = get_img(img_name, bounding_box, image_size)\n",
        "\n",
        "            all_embeddings1 = all_embeddings[index, :, :]\n",
        "\n",
        "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
        "            embedding = all_embeddings1[embedding_ix, :]\n",
        "\n",
        "            X.append(np.array(img))\n",
        "            y.append(class_ids[index])\n",
        "            embeddings.append(embedding)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    embeddings = np.array(embeddings)\n",
        "    return X, y, embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ODvYdkGtpGKJ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "__1ejH3mpFon"
      },
      "outputs": [],
      "source": [
        "def generate_c(x):\n",
        "    mean = x[:, :128]\n",
        "    log_sigma = x[:, 128:]\n",
        "    stddev = K.exp(log_sigma)\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
        "    c = stddev * epsilon + mean\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LYGM28snpPMb"
      },
      "outputs": [],
      "source": [
        "def build_ca_model():\n",
        "    \"\"\"\n",
        "    Get conditioning augmentation model.\n",
        "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jY_cMcEbpRSW"
      },
      "outputs": [],
      "source": [
        "def build_embedding_compressor_model():\n",
        "    \"\"\"\n",
        "    Build embedding compressor model\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N5BK6rWkpSyQ"
      },
      "outputs": [],
      "source": [
        "def build_stage1_generator():\n",
        "    \"\"\"\n",
        "    Builds a generator model used in Stage-I\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = UpSampling2D(size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation(activation='tanh')(x)\n",
        "\n",
        "    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n",
        "    return stage1_gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xJLclqSTpVGj"
      },
      "outputs": [],
      "source": [
        "def build_stage1_discriminator():\n",
        "    \"\"\"\n",
        "    Create a model which takes two inputs\n",
        "    1. One from above network\n",
        "    2. One from the embedding layer\n",
        "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=(64, 64, 3))\n",
        "\n",
        "    x = Conv2D(64, (4, 4),\n",
        "               padding='same', strides=2,\n",
        "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    input_layer2 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    merged_input = concatenate([x, input_layer2])\n",
        "\n",
        "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
        "                padding=\"same\", strides=1)(merged_input)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
        "    x2 = Flatten()(x2)\n",
        "    x2 = Dense(1)(x2)\n",
        "    x2 = Activation('sigmoid')(x2)\n",
        "\n",
        "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n",
        "    return stage1_dis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NREfScoDpXKb"
      },
      "outputs": [],
      "source": [
        "def build_adversarial_model(gen_model, dis_model):\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "    input_layer3 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    x, mean_logsigma = gen_model([input_layer, input_layer2])\n",
        "\n",
        "    dis_model.trainable = False\n",
        "    valid = dis_model([x, input_layer3])\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eNBHRmpqpZgo"
      },
      "source": [
        "# Defining Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "awQ2XK6lpZDr"
      },
      "outputs": [],
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "S6wkVR9lpd5q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_generator_loss(y_true, y_pred):\n",
        "    # Calculate binary cross entropy loss\n",
        "    return K.binary_crossentropy(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1wGTNAxZpfoc"
      },
      "outputs": [],
      "source": [
        "def save_rgb_img(img, path):\n",
        "    \"\"\"\n",
        "    Save an rgb image\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Image\")\n",
        "\n",
        "    plt.savefig(path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vfdGkJj0phPb"
      },
      "outputs": [],
      "source": [
        "def write_log(callback, name, loss, batch_no):\n",
        "    \"\"\"\n",
        "    Write training summary to TensorBoard\n",
        "    \"\"\"\n",
        "    summary = tf.Summary()\n",
        "    summary_value = summary.value.add()\n",
        "    summary_value.simple_value = loss\n",
        "    summary_value.tag = name\n",
        "    callback.writer.add_summary(summary, batch_no)\n",
        "    callback.writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qTgvkqyvptfn"
      },
      "source": [
        "# Downloading of dataset in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "HFIC465b2w0u",
        "outputId": "030d80eb-24f4-4742-f068-b3f75b9ef7f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "rdxKQ0663T_C",
        "outputId": "886fbb23-0435-4b08-cbe4-5801e0afcdf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eHohaW6L2wyW"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'CUB_200_2011.tgz'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tar \u001b[38;5;241m=\u001b[39m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCUB_200_2011.tgz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tar\u001b[38;5;241m.\u001b[39mextractall()\n\u001b[0;32m      4\u001b[0m tar\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\generalEnv\\Lib\\tarfile.py:1835\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     saved_pos \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ReadError, CompressionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1837\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomptype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\generalEnv\\Lib\\tarfile.py:1903\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip module is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1903\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\generalEnv\\Lib\\gzip.py:192\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    190\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CUB_200_2011.tgz'"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"CUB_200_2011.tgz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "jZ6Xgi--2wvp",
        "outputId": "59f7ebfe-40cf-4cfe-f8ff-62a0cc3bd303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adc.json\tbirds.zip  cub\t\t     cub.tar.gz  results\n",
            "attributes.txt\tcoco\t   CUB_200_2011      logs\t sample_data\n",
            "birds\t\tcoco.zip   CUB_200_2011.tgz  __MACOSX\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "0fTLsFUb4ftd",
        "outputId": "b8ce771b-a488-43ac-a587-dccfe98c3106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CUB_200_2011\n"
          ]
        }
      ],
      "source": [
        "cd CUB_200_2011"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "4_xbNKn-4f5r",
        "outputId": "0785899d-b37f-4342-d211-c7a6787d4cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attributes\t    image_class_labels.txt  parts\n",
            "bounding_boxes.txt  images\t\t    README\n",
            "classes.txt\t    images.txt\t\t    train_test_split.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AdEAGWQE4gB-"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "9BMrVDAC4gMN",
        "outputId": "04657c0c-fb68-4c5e-c805-a1f1f1054845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adc.json\tbirds.zip  cub\t\t     cub.tar.gz  results\n",
            "attributes.txt\tcoco\t   CUB_200_2011      logs\t sample_data\n",
            "birds\t\tcoco.zip   CUB_200_2011.tgz  __MACOSX\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JWRKO6IfpkFt"
      },
      "source": [
        "# Main File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6Pjt8uWEpjsS"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "        # Define local paths\n",
        "        data_dir = \"./birds/\"  # Change this to your local birds directory path\n",
        "        train_dir = os.path.join(data_dir, \"train\")\n",
        "        test_dir = os.path.join(data_dir, \"test\")\n",
        "        cub_dataset_dir = \"./CUB_200_2011\"  # Change this to your local CUB dataset directory path\n",
        "        \n",
        "        # Create results directory if it doesn't exist\n",
        "        if not os.path.exists(\"results\"):\n",
        "            os.makedirs(\"results\")\n",
        "            \n",
        "        # Model parameters\n",
        "        image_size = 64\n",
        "        batch_size = 64\n",
        "        z_dim = 100\n",
        "        stage1_generator_lr = 0.0002\n",
        "        stage1_discriminator_lr = 0.0002\n",
        "        stage1_lr_decay_step = 600\n",
        "        epochs = 1000\n",
        "        condition_dim = 128\n",
        "\n",
        "        # File paths\n",
        "        embeddings_file_path_train = os.path.join(train_dir, \"char-CNN-RNN-embeddings.pickle\")\n",
        "        embeddings_file_path_test = os.path.join(test_dir, \"char-CNN-RNN-embeddings.pickle\")\n",
        "        filenames_file_path_train = os.path.join(train_dir, \"filenames.pickle\")\n",
        "        filenames_file_path_test = os.path.join(test_dir, \"filenames.pickle\")\n",
        "        class_info_file_path_train = os.path.join(train_dir, \"class_info.pickle\")\n",
        "        class_info_file_path_test = os.path.join(test_dir, \"class_info.pickle\")\n",
        "\n",
        "        # Define optimizers\n",
        "        dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "        gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "        # Create logs directory if it doesn't exist\n",
        "        if not os.path.exists(\"logs\"):\n",
        "            os.makedirs(\"logs\")\n",
        "\n",
        "        \"\"\"\n",
        "        Load datasets\n",
        "        \"\"\"\n",
        "        print(\"Loading training data...\")\n",
        "        X_train, y_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
        "                                                      class_info_file_path=class_info_file_path_train,\n",
        "                                                      cub_dataset_dir=cub_dataset_dir,\n",
        "                                                      embeddings_file_path=embeddings_file_path_train,\n",
        "                                                      image_size=(64, 64))\n",
        "\n",
        "        print(\"Loading test data...\")\n",
        "        X_test, y_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
        "                                                   class_info_file_path=class_info_file_path_test,\n",
        "                                                   cub_dataset_dir=cub_dataset_dir,\n",
        "                                                   embeddings_file_path=embeddings_file_path_test,\n",
        "                                                   image_size=(64, 64))\n",
        "\n",
        "        \"\"\"\n",
        "        Build and compile networks\n",
        "        \"\"\"\n",
        "        print(\"Building models...\")\n",
        "        ca_model = build_ca_model()\n",
        "        ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "        stage1_dis = build_stage1_discriminator()\n",
        "        stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "        stage1_gen = build_stage1_generator()\n",
        "        stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
        "\n",
        "        embedding_compressor_model = build_embedding_compressor_model()\n",
        "        embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "        adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
        "        adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
        "                              optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "        # Setup TensorBoard\n",
        "        log_dir = os.path.join(\"logs\", str(int(time.time())))\n",
        "        tensorboard = TensorBoard(log_dir=log_dir)\n",
        "        tensorboard.set_model(stage1_gen)\n",
        "        tensorboard.set_model(stage1_dis)\n",
        "        tensorboard.set_model(ca_model)\n",
        "        tensorboard.set_model(embedding_compressor_model)\n",
        "\n",
        "        # Training loop\n",
        "        real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
        "        fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"========================================\")\n",
        "            print(\"Epoch is:\", epoch)\n",
        "            print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
        "\n",
        "            gen_losses = []\n",
        "            dis_losses = []\n",
        "\n",
        "            number_of_batches = int(X_train.shape[0] / batch_size)\n",
        "            for index in range(number_of_batches):\n",
        "                print(\"Batch:{}\".format(index+1))\n",
        "                \n",
        "                # Rest of the training code remains the same\n",
        "                # ... existing training loop code ...\n",
        "\n",
        "            # Save models periodically\n",
        "            if epoch % 10 == 0:\n",
        "                stage1_gen.save_weights(f\"stage1_gen_epoch_{epoch}.h5\")\n",
        "                stage1_dis.save_weights(f\"stage1_dis_epoch_{epoch}.h5\")\n",
        "\n",
        "        # Save final models\n",
        "        stage1_gen.save_weights(\"stage1_gen.h5\")\n",
        "        stage1_dis.save_weights(\"stage1_dis.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QCYyOZMRXzfv"
      },
      "outputs": [],
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ot1xZ3odm6eI"
      },
      "outputs": [],
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "nGYJWLTGm9R4",
        "outputId": "3192cc05-1a55-4a8e-9f5f-1e13d6d90683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1mzYY3DjNTzxlilPfsIo_hNt0SW3Bba-K\n"
          ]
        }
      ],
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'stage1_dis.h5'})\n",
        "uploaded.SetContentFile('stage1_dis.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "glMY1sSNnFQi",
        "outputId": "f71faa05-684d-486f-99fd-d8da179efdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1j6UN3VOUMg-chLY0Pf5lfiVcQFKmSei-\n"
          ]
        }
      ],
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'stage1_gen.h5'})\n",
        "uploaded.SetContentFile('stage1_gen.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nfRs1DsqnJVc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "psC9UHjzliA1",
        "XZybHodapH3H",
        "ODvYdkGtpGKJ",
        "eNBHRmpqpZgo"
      ],
      "include_colab_link": true,
      "name": "stackgan_stage_I_imp.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "generalEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
